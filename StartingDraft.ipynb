{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc9208a0",
   "metadata": {},
   "source": [
    "## Start with a markov model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5943ae6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03810539, 0.        , 0.49523234, 0.46666227, 0.        ])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# position 0 is always an absorbing state\n",
    "\n",
    "# a transition matrix would satisfy, each row sums to 1\n",
    "\n",
    "# for convenience, N will indicate N potential products, but the actual matrix will be (n+1)*(n+1) due to potision 0\n",
    "def gen_transition(N):\n",
    "    base = np.identity(N+1)\n",
    "    \n",
    "    sub_M = np.random.uniform(low=0., high=1, size=(N, N))\n",
    "    \n",
    "    sub_M[sub_M < 1 - 1/np.sqrt(N)]=0   # to maintain a sparse structure\n",
    "    \n",
    "    base[1:N+1,1:N+1] = sub_M\n",
    "    \n",
    "    base[1:N+1,0] = np.random.uniform(low=0.1, high=0.3, size = N)\n",
    "    \n",
    "    base = base / base.sum(axis=1, keepdims = True)\n",
    "    \n",
    "    return base\n",
    "\n",
    "# share the probability on diagonal to other places\n",
    "def gen_transition_nodiag(N):\n",
    "    mat = gen_transition(N)\n",
    "    for i in range(1,N+1):\n",
    "        mat[i,i]=0\n",
    "        \n",
    "    mat = mat / mat.sum(axis=1, keepdims = True)\n",
    "    return mat\n",
    "\n",
    "# utilitiy, from probability vector to index_indicator\n",
    "def indicator(probs):\n",
    "    if not sum(probs) > 1.0 - 1e-6 and sum(probs) < 1.0 + 1e-6:\n",
    "        print(\"WRONG PROBABILITY!\")\n",
    "        return\n",
    "    index = np.random.choice(len(probs), 1, p=probs)[0]\n",
    "    indicate = np.zeros(len(probs))\n",
    "    indicate[index] = 1\n",
    "    return indicate\n",
    "\n",
    "\n",
    "# transition matrix should be given in advance, for sure!\n",
    "# given an assortment, generate a hitting sample\n",
    "def single_sample_generator(Lams, TransP, Assorts):\n",
    "    Lams = np.insert(Lams, 0, 0)\n",
    "    Assorts = np.insert(Assorts, 0, 0)\n",
    "    choice = indicator(Lams)\n",
    "    while True:\n",
    "        if choice[0] == 1 or np.dot(choice, Assorts)>0:\n",
    "            return choice\n",
    "        else:\n",
    "            choice_prob = TransP[np.squeeze(np.argwhere(choice == 1))]\n",
    "            choice = indicator(choice_prob)\n",
    "\n",
    "def multi_sample_generator(Lams, TransP, Assorts, sampleN):\n",
    "    N = len(Lams)\n",
    "    choices = np.zeros((sampleN,N+1))\n",
    "    for i in range(sampleN):\n",
    "        choices[i] = single_sample_generator(Lams, TransP, Assorts)\n",
    "        \n",
    "    return choices\n",
    "    \n",
    "# the exact probability for product choice    \n",
    "def actual_prob(Lams, TransP, Assorts):\n",
    "    Lams = np.insert(Lams, 0, 0)\n",
    "    Assorts = np.insert(Assorts, 0, 1)\n",
    "    S_plus = np.squeeze(np.argwhere(Assorts == 1),axis=1)\n",
    "    S_bar = np.squeeze(np.argwhere(Assorts == 0),axis=1)\n",
    "    B = TransP[np.expand_dims(S_bar, axis=1), S_plus]\n",
    "    C = TransP[np.expand_dims(S_bar, axis=1), S_bar]\n",
    "    \n",
    "    distri = np.zeros(len(Lams))\n",
    "    \n",
    "    addi = np.matmul(np.matmul(np.expand_dims(Lams[S_bar], axis=0), np.linalg.inv(np.identity(len(C)) - C)), B)\n",
    "    \n",
    "    count = 0\n",
    "    for i in S_plus:\n",
    "        distri[i] = Lams[i] + addi[0,count]\n",
    "        count += 1\n",
    "    \n",
    "    return distri\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "transP = gen_transition_nodiag(4)\n",
    "lams = np.array([0.1,0.4,0.3,0.2])\n",
    "assortment = np.array([0,1,1,0])\n",
    "# bundle = np.squeeze(np.argwhere(assortment == 1))\n",
    "actual_prob(lams, transP, assortment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28c0e88",
   "metadata": {},
   "source": [
    "##  We use multi_sample_generator to test actual_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2f7e492a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. 1. 0. 1. 1. 0. 1. 0.]\n",
      "[0.10532507 0.         0.26029582 0.         0.18388993 0.\n",
      " 0.15390964 0.13728175 0.         0.1592978  0.        ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1031.,    0., 2596.,    0., 1824.,    0., 1536., 1431.,    0.,\n",
       "       1582.,    0.])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transP = gen_transition_nodiag(10)\n",
    "lams = np.array([0.1]*10)\n",
    "assortment = np.zeros(10)\n",
    "assortment[[1,3,5,6,8]]=1\n",
    "print(assortment)\n",
    "\n",
    "# bundle = np.squeeze(np.argwhere(assortment == 1))\n",
    "print(actual_prob(lams, transP, assortment))\n",
    "\n",
    "data = np.sum(multi_sample_generator(lams, transP, assortment, 10000), axis=0)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f67c1b",
   "metadata": {},
   "source": [
    "## After the checks above, we start building the network to check its representation power"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa644a69",
   "metadata": {},
   "source": [
    "### start with some experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1742802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.2530, -1.5013],\n",
      "          [-1.2542,  1.4800]],\n",
      "\n",
      "         [[-0.3728,  1.1849],\n",
      "          [-0.7995, -1.5207]],\n",
      "\n",
      "         [[ 0.3995,  1.7606],\n",
      "          [ 0.0475,  0.0821]]],\n",
      "\n",
      "\n",
      "        [[[-0.1104,  0.3872],\n",
      "          [ 1.4053,  0.4585]],\n",
      "\n",
      "         [[-0.0354, -1.5229],\n",
      "          [ 0.5594, -1.9008]],\n",
      "\n",
      "         [[-0.1361,  0.4570],\n",
      "          [ 0.0560,  1.1091]]]])\n",
      "[tensor([1., 1., 1.]), tensor([0., 0., 0.])]\n",
      "tensor([[[[ 0.1114, -1.6138],\n",
      "          [-1.3708,  1.3180]],\n",
      "\n",
      "         [[ 0.1748,  1.7029],\n",
      "          [-0.2438, -0.9513]],\n",
      "\n",
      "         [[-0.1194,  2.1232],\n",
      "          [-0.6994, -0.6424]]],\n",
      "\n",
      "\n",
      "        [[[-0.2460,  0.2433],\n",
      "          [ 1.2445,  0.3134]],\n",
      "\n",
      "         [[ 0.5058, -0.9535],\n",
      "          [ 1.0893, -1.3242]],\n",
      "\n",
      "         [[-1.0018, -0.0247],\n",
      "          [-0.6853,  1.0497]]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "1.4901161e-08\n",
      "0.9999904\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# batch normalization\n",
    "example = torch.randn((2,3,2,2))\n",
    "print(example)\n",
    "batch_layer = nn.BatchNorm2d(3)\n",
    "print([para.data for para in batch_layer.parameters()])\n",
    "output=batch_layer(example)\n",
    "print(output)\n",
    "channel0 = output[:,0,:,:].detach().numpy()\n",
    "print(np.mean(channel0))\n",
    "print(np.var(channel0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0cef4116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 1.6710,  0.0407],\n",
      "          [ 1.0089,  0.6703]],\n",
      "\n",
      "         [[ 0.7476, -0.5383],\n",
      "          [-0.4221, -1.3066]],\n",
      "\n",
      "         [[-1.6165,  0.7378],\n",
      "          [-0.9020, -0.2693]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0502,  0.5441],\n",
      "          [ 1.3378, -0.4834]],\n",
      "\n",
      "         [[-0.1206, -1.2273],\n",
      "          [-0.5006,  0.2532]],\n",
      "\n",
      "         [[ 2.1000, -0.8446],\n",
      "          [ 0.9251,  1.0094]]]])\n",
      "tensor([[[[ 1.7579e+00,  5.7929e-02],\n",
      "          [ 1.0676e+00,  7.1446e-01]],\n",
      "\n",
      "         [[ 7.9507e-01, -5.4582e-01],\n",
      "          [-4.2461e-01, -1.3470e+00]],\n",
      "\n",
      "         [[-1.6701e+00,  7.8486e-01],\n",
      "          [-9.2507e-01, -2.6531e-01]]],\n",
      "\n",
      "\n",
      "        [[[-2.1863e-01,  3.1225e-01],\n",
      "          [ 1.1655e+00, -7.9228e-01]],\n",
      "\n",
      "         [[-4.0226e-01, -1.5920e+00],\n",
      "          [-8.1082e-01, -4.3100e-04]],\n",
      "\n",
      "         [[ 1.9849e+00, -1.1806e+00],\n",
      "          [ 7.2190e-01,  8.1247e-01]]]], grad_fn=<NativeLayerNormBackward0>)\n",
      "1.4901161e-08\n",
      "0.9999893\n"
     ]
    }
   ],
   "source": [
    "example = torch.randn((2,3,2,2))\n",
    "print(example)\n",
    "layer_norm = nn.LayerNorm((3,2,2))\n",
    "output=layer_norm(example)\n",
    "print(output)\n",
    "batch0 = output[0,:,:,:].detach().numpy()\n",
    "print(np.mean(batch0))\n",
    "print(np.var(batch0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167c6482",
   "metadata": {},
   "source": [
    "## consider a bottle neck structure, input assortment, output probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8616ef89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5822, 0.5352, 0.6441, 0.6425, 0.5476, 0.6013, 0.5168, 0.5637, 0.6328,\n",
      "         0.5288],\n",
      "        [0.5577, 0.6424, 0.5850, 0.6123, 0.6437, 0.6302, 0.6377, 0.5379, 0.5141,\n",
      "         0.6415],\n",
      "        [0.6441, 0.6135, 0.5553, 0.5354, 0.5946, 0.5161, 0.6296, 0.6443, 0.6351,\n",
      "         0.5870]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    \n",
    "    def __init__(self, Veclen, Necklen):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(Veclen, Necklen)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(Necklen)\n",
    "        \n",
    "        self.fc2 = nn.Linear(Necklen, Necklen)\n",
    "        \n",
    "        self.bn2 = nn.BatchNorm1d(Necklen)\n",
    "        \n",
    "        self.fc3 = nn.Linear(Necklen, Veclen)\n",
    "        \n",
    "        self.bn3 = nn.BatchNorm1d(Veclen)\n",
    "        \n",
    "        self.relu = nn.LeakyReLU(0.2, inplace=True)\n",
    "        \n",
    "        self.fc4 = nn.Linear(2,1)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = torch.unsqueeze(x, 1)\n",
    "\n",
    "        out = self.fc1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.fc2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.fc3(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = torch.unsqueeze(out, 1)\n",
    "        \n",
    "        out = torch.cat((out,residual),1).permute(0,2,1)\n",
    "        \n",
    "        out = torch.squeeze(self.fc4(out),2)\n",
    "        \n",
    "        out = self.sigmoid(out)\n",
    "\n",
    "        return out\n",
    "        \n",
    "N = 10\n",
    "neck = 4\n",
    "batchsize = 3\n",
    "lams = np.array([0.1]*10)\n",
    "\n",
    "sample = np.zeros((batchsize, N))\n",
    "for i in range(batchsize):\n",
    "    sample[i] = indicator(lams)\n",
    "\n",
    "inp = torch.tensor(sample).float()\n",
    "\n",
    "net = Bottleneck(N,neck)\n",
    "print(net(inp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a49933d",
   "metadata": {},
   "source": [
    "## training, with synthetic data generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f4f92208",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x25ad483fc40>]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N_prod = 10\n",
    "Vec_Len = N_prod + 1\n",
    "Neck_Len = 5\n",
    "EPOCHS = 1000\n",
    "\n",
    "transP = gen_transition_nodiag(N_prod)\n",
    "lams = np.array([1/N_prod] * N_prod)\n",
    "Training_Sample_Amount = 10000\n",
    "\n",
    "INPUT = np.zeros((Training_Sample_Amount,Vec_Len))\n",
    "OUTPUT = np.zeros((Training_Sample_Amount,Vec_Len))\n",
    "for i in range(Training_Sample_Amount):\n",
    "    # generate input vector, which has length vec_len\n",
    "    # the first element is 0, which should be ignored when treated as an assortment\n",
    "    potential_vec = np.random.uniform(low=0., high=1, size=Vec_Len)\n",
    "    potential_vec[0] = 0\n",
    "    potential_vec[potential_vec > 1/2] = 1\n",
    "    potential_vec[potential_vec <= 1/2] = 0\n",
    "    \n",
    "    INPUT[i] = potential_vec\n",
    "    OUTPUT[i] = actual_prob(lams, transP, potential_vec[1:])\n",
    "    \n",
    "created_dataset = TensorDataset(torch.Tensor(INPUT),torch.Tensor(OUTPUT))    \n",
    "dataloader = DataLoader(created_dataset, shuffle=True, batch_size = 500)\n",
    "\n",
    "\n",
    "model = Bottleneck(Vec_Len, Neck_Len)\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0005)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "\n",
    "losses = []\n",
    "for epo in range(EPOCHS):\n",
    "    print(epo)\n",
    "    for step, (IN, TEST) in enumerate(dataloader):\n",
    "        OUT = model(IN)\n",
    "        loss = criterion(OUT,TEST)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.detach().item())\n",
    "    \n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fe48b6",
   "metadata": {},
   "source": [
    "## after the training, we try to predict a guy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8f5b3fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.19333474 0.1        0.23349433 0.         0.         0.\n",
      " 0.16882909 0.20434184 0.         0.1        0.        ]\n",
      "[2.7229998e-01 1.6440183e-01 1.1338722e-02 2.0713727e-03 2.6742101e-03\n",
      " 2.4091788e-03 1.8517564e-01 2.1767445e-01 5.9548969e-05 4.6196301e-03\n",
      " 1.4507346e-04]\n"
     ]
    }
   ],
   "source": [
    "Assort_Instance = np.array([1,1,0,0,1,1,1,1,1,0])\n",
    "\n",
    "\n",
    "\n",
    "actu_prob = actual_prob(lams, transP, Assort_Instance)\n",
    "print(actu_prob)\n",
    "\n",
    "IN_VEC = torch.Tensor(\n",
    "np.array([\n",
    "    [0,1,1,0,0,0,1,1,0,1,0],\n",
    "    [0,0,1,1,0,1,0,1,0,0,1]\n",
    "])\n",
    ")\n",
    "\n",
    "OUT = model(IN_VEC)\n",
    "print(OUT[0].detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac6015d",
   "metadata": {},
   "source": [
    "## somehow, looks, reasonable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfde89d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047d7f90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
